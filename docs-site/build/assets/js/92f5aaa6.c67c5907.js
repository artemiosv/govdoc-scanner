"use strict";(self.webpackChunkdocs_site=self.webpackChunkdocs_site||[]).push([[738],{1935:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"gsoc/2025/overview","title":"GSoC 2025 Project Overview: GovDoc Scanner","description":"Mentors: Giannis E. Skitsas, Vasilis Christopoulos","source":"@site/docs/gsoc/2025/overview.md","sourceDirName":"gsoc/2025","slug":"/gsoc/2025/overview","permalink":"/govdoc-scanner/docs/gsoc/2025/overview","draft":false,"unlisted":false,"editUrl":"https://github.com/flexivian/govdoc-scanner/edit/main/docs-site/docs/gsoc/2025/overview.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docsSidebar","previous":{"title":"Code Examples","permalink":"/govdoc-scanner/docs/code-examples/overview"}}');var i=o(4848),s=o(8453);const r={sidebar_position:1},a="GSoC 2025 Project Overview: GovDoc Scanner",c={},l=[{value:"Abstract",id:"abstract",level:2},{value:"Main goals for GSoC 2025",id:"main-goals-for-gsoc-2025",level:2},{value:"Implementation",id:"implementation",level:2},{value:"Crawler Application",id:"crawler-application",level:3},{value:"Document Scanner Application",id:"document-scanner-application",level:3},{value:"Orchestrator Script",id:"orchestrator-script",level:3},{value:"Monorepo and Development",id:"monorepo-and-development",level:3},{value:"Repository",id:"repository",level:2},{value:"Quick Start",id:"quick-start",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"gsoc-2025-project-overview-govdoc-scanner",children:"GSoC 2025 Project Overview: GovDoc Scanner"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Mentors"}),": Giannis E. Skitsas, Vasilis Christopoulos"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Assignee"}),": Eftihis Drakakis"]}),"\n",(0,i.jsx)(n.h2,{id:"abstract",children:"Abstract"}),"\n",(0,i.jsx)(n.p,{children:"The GovDoc Scanner project was undertaken to address the challenge of accessing and utilizing public company data from the Greek GEMI portal, which is often locked away in unstructured PDF and DOCX files. The project successfully developed a suite of tools to automate the process of fetching, processing, and structuring this data. The outcome is a powerful system that can crawl the GEMI portal, download relevant documents, extract metadata and content using generative AI, and organize the information into a structured, searchable format. This work significantly improves the transparency and accessibility of public corporate information in Greece."}),"\n",(0,i.jsx)(n.h2,{id:"main-goals-for-gsoc-2025",children:"Main goals for GSoC 2025"}),"\n",(0,i.jsx)(n.p,{children:"The primary objectives for this project were:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Automated Document Crawling"}),": Develop a robust crawler to navigate the GEMI portal, search for companies using various filters, and download all associated public documents."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Intelligent Document Processing"}),": Create a document processing pipeline that can handle different file formats (PDF, DOCX, DOC), extract text, and use Google's Gemini AI to extract structured metadata and generate contextual summaries."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"End-to-End Orchestration"}),": Build a top-level script to automate the entire workflow, from crawling and downloading to processing and storing the data, complete with progress tracking and error handling."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Comprehensive Documentation"}),": Establish a documentation site to provide clear instructions for installation, usage, and development, ensuring the project is accessible to a wide audience."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"implementation",children:"Implementation"}),"\n",(0,i.jsx)(n.h3,{id:"crawler-application",children:"Crawler Application"}),"\n",(0,i.jsx)(n.p,{children:"The crawler is a Node.js application responsible for interacting with the GEMI portal."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Web Scraping and Automation"}),": ",(0,i.jsx)(n.code,{children:"Playwright"})," is used for browser automation, enabling the crawler to perform complex interactions like filling out forms and navigating through pages to find and download documents. ",(0,i.jsx)(n.code,{children:"Cheerio"})," is used for parsing the HTML of the portal to extract links and other relevant information."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"User Interface"}),": An interactive command-line interface (CLI) was built using ",(0,i.jsx)(n.code,{children:"inquirer"}),", allowing users to easily specify which companies to search for or which GEMI IDs to download documents for."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"HTTP Requests and Data Handling"}),": ",(0,i.jsx)(n.code,{children:"axios"})," is used for making direct HTTP requests to download files, and ",(0,i.jsx)(n.code,{children:"greek-utils"})," helps in handling Greek-specific text and character sets. ",(0,i.jsx)(n.code,{children:"string-similarity"})," is used to find potential matches for company names."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"document-scanner-application",children:"Document Scanner Application"}),"\n",(0,i.jsx)(n.p,{children:"The doc-scanner application processes the downloaded documents to extract valuable information."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AI-Powered Metadata Extraction"}),": The core of the scanner uses the ",(0,i.jsx)(n.code,{children:"@google/generative-ai"})," package to communicate with the Google Gemini API. This allows for sophisticated analysis of the document text to extract metadata and generate contextual histories."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Document Text Extraction"}),": The application supports multiple document formats. ",(0,i.jsx)(n.code,{children:"mammoth"})," is used to extract raw text from ",(0,i.jsx)(n.code,{children:".docx"})," files, and ",(0,i.jsx)(n.code,{children:"word-extractor"})," handles older ",(0,i.jsx)(n.code,{children:".doc"})," files."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Asynchronous Processing"}),": To handle large numbers of documents efficiently, ",(0,i.jsx)(n.code,{children:"p-limit"})," is used to control the concurrency of asynchronous operations, preventing the application from overwhelming the system or hitting API rate limits."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Environment Management"}),": The ",(0,i.jsx)(n.code,{children:"dotenv"})," package is used to manage environment variables, keeping sensitive information like API keys out of the source code."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"orchestrator-script",children:"Orchestrator Script"}),"\n",(0,i.jsx)(n.p,{children:"The orchestrator script ties the crawler and the doc-scanner together into a seamless workflow."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Workflow Automation"}),": This script automates the process of running the crawler to download documents and then passing them to the doc-scanner for processing."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Progress Tracking"}),": ",(0,i.jsx)(n.code,{children:"cli-progress"})," is used to display a progress bar in the terminal, giving users real-time feedback on the status of the batch processing."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"monorepo-and-development",children:"Monorepo and Development"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Nx Monorepo"}),": The entire project is managed as a monorepo using ",(0,i.jsx)(n.code,{children:"Nx"}),". This simplifies the management of the different applications (",(0,i.jsx)(n.code,{children:"crawler"}),", ",(0,i.jsx)(n.code,{children:"doc-scanner"}),") and shared scripts, and helps in enforcing consistent development practices."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Playwright"}),": ",(0,i.jsx)(n.code,{children:"playwright"})," is also used for end-to-end testing of the applications."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"repository",children:"Repository"}),"\n",(0,i.jsxs)(n.p,{children:["The repository for this project can be found ",(0,i.jsx)(n.a,{href:"https://github.com/flexivian/govdoc-scanner",children:"here"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Development"}),": Setup the development environment as described in the ",(0,i.jsx)(n.a,{href:"/govdoc-scanner/docs/installation/Development",children:"Development"})," guide."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Documentation"}),": The documentation site can be started by following the instructions in the ",(0,i.jsx)(n.a,{href:"/govdoc-scanner/docs/installation/Production",children:"Production"})," guide."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>a});var t=o(6540);const i={},s=t.createContext(i);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);